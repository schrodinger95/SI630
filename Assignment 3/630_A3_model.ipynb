{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>reply_text</th>\n",
       "      <th>rlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_n27vu3</td>\n",
       "      <td>What's something nice you like to do just to b...</td>\n",
       "      <td>gwhrhmf</td>\n",
       "      <td>Give compliments. It’s extremely easy to do an...</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_n2az7m</td>\n",
       "      <td>So what is the best headphones for people who ...</td>\n",
       "      <td>gwiatps</td>\n",
       "      <td>I prefer Raycon Performance Ear Buds. They are...</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_n2dzr9</td>\n",
       "      <td>How do you go on knowing a loved one only has ...</td>\n",
       "      <td>gwit1wj</td>\n",
       "      <td>Make it as memorable as the rest of your time ...</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_n2iy9q</td>\n",
       "      <td>You’ve been dropped to the year 1800 with all ...</td>\n",
       "      <td>gwjhw8i</td>\n",
       "      <td>They're gonna burn me at the stake for being a...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_n2kuuq</td>\n",
       "      <td>Stuck in bad habits for years and I realized i...</td>\n",
       "      <td>gwkiiie</td>\n",
       "      <td>Using new environments as a way to create heal...</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>t3_njln4h</td>\n",
       "      <td>What is the first thing that comes to mind whe...</td>\n",
       "      <td>gz82i2v</td>\n",
       "      <td>Austin, we have a problem. Just watched an epi...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>t3_nkx9c8</td>\n",
       "      <td>What career advice would you give to someone w...</td>\n",
       "      <td>gzfan0b</td>\n",
       "      <td>Work anywhere while you build qualifications a...</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>t3_nm2mc0</td>\n",
       "      <td>What was your most unexplainable and bizarre e...</td>\n",
       "      <td>gzm4krx</td>\n",
       "      <td>Doing my homework at the desk when I was like ...</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>t3_nnbl3i</td>\n",
       "      <td>Do other countries have as much coverage of UF...</td>\n",
       "      <td>gztlr9g</td>\n",
       "      <td>In Mexico, there’s barely any news if any but ...</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>t3_nojmhe</td>\n",
       "      <td>What is the worst present you have ever receiv...</td>\n",
       "      <td>h00ehvy</td>\n",
       "      <td>NOPE I have a different answer! My father's le...</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_id                                      question_text reply_id  \\\n",
       "0      t3_n27vu3  What's something nice you like to do just to b...  gwhrhmf   \n",
       "1      t3_n2az7m  So what is the best headphones for people who ...  gwiatps   \n",
       "2      t3_n2dzr9  How do you go on knowing a loved one only has ...  gwit1wj   \n",
       "3      t3_n2iy9q  You’ve been dropped to the year 1800 with all ...  gwjhw8i   \n",
       "4      t3_n2kuuq  Stuck in bad habits for years and I realized i...  gwkiiie   \n",
       "...          ...                                                ...      ...   \n",
       "5395   t3_njln4h  What is the first thing that comes to mind whe...  gz82i2v   \n",
       "5396   t3_nkx9c8  What career advice would you give to someone w...  gzfan0b   \n",
       "5397   t3_nm2mc0  What was your most unexplainable and bizarre e...  gzm4krx   \n",
       "5398   t3_nnbl3i  Do other countries have as much coverage of UF...  gztlr9g   \n",
       "5399   t3_nojmhe  What is the worst present you have ever receiv...  h00ehvy   \n",
       "\n",
       "                                             reply_text  rlen  \n",
       "0     Give compliments. It’s extremely easy to do an...   205  \n",
       "1     I prefer Raycon Performance Ear Buds. They are...   178  \n",
       "2     Make it as memorable as the rest of your time ...   278  \n",
       "3     They're gonna burn me at the stake for being a...    52  \n",
       "4     Using new environments as a way to create heal...   651  \n",
       "...                                                 ...   ...  \n",
       "5395  Austin, we have a problem. Just watched an epi...   168  \n",
       "5396  Work anywhere while you build qualifications a...   449  \n",
       "5397  Doing my homework at the desk when I was like ...   348  \n",
       "5398  In Mexico, there’s barely any news if any but ...   377  \n",
       "5399  NOPE I have a different answer! My father's le...   988  \n",
       "\n",
       "[5400 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"si630w22-hw3-data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_n27vu3</td>\n",
       "      <td>user_00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>group_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_n27vu3</td>\n",
       "      <td>user_01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>group_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_n27vu3</td>\n",
       "      <td>user_02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>group_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_n2az7m</td>\n",
       "      <td>user_00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>group_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_n2az7m</td>\n",
       "      <td>user_01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>group_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17840</th>\n",
       "      <td>t3_np0hd0</td>\n",
       "      <td>user_57</td>\n",
       "      <td>2.0</td>\n",
       "      <td>group_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17841</th>\n",
       "      <td>t3_np0hd0</td>\n",
       "      <td>user_58</td>\n",
       "      <td>4.0</td>\n",
       "      <td>group_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17842</th>\n",
       "      <td>t3_np0myc</td>\n",
       "      <td>user_56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>group_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17843</th>\n",
       "      <td>t3_np0myc</td>\n",
       "      <td>user_57</td>\n",
       "      <td>4.0</td>\n",
       "      <td>group_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17844</th>\n",
       "      <td>t3_np0myc</td>\n",
       "      <td>user_58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>group_11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17845 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id annotator_id  rating     group\n",
       "0      t3_n27vu3      user_00     5.0  group_09\n",
       "1      t3_n27vu3      user_01     5.0  group_09\n",
       "2      t3_n27vu3      user_02     5.0  group_09\n",
       "3      t3_n2az7m      user_00     5.0  group_09\n",
       "4      t3_n2az7m      user_01     5.0  group_09\n",
       "...          ...          ...     ...       ...\n",
       "17840  t3_np0hd0      user_57     2.0  group_11\n",
       "17841  t3_np0hd0      user_58     4.0  group_11\n",
       "17842  t3_np0myc      user_56     1.0  group_11\n",
       "17843  t3_np0myc      user_57     4.0  group_11\n",
       "17844  t3_np0myc      user_58     2.0  group_11\n",
       "\n",
       "[17845 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"si630w22-hw3-train.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's something nice you like to do just to b...</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So what is the best headphones for people who ...</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you go on knowing a loved one only has ...</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You’ve been dropped to the year 1800 with all ...</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stuck in bad habits for years and I realized i...</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>Is this normal? Should I say something? From a...</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>If Gender is how you feel, and not what your b...</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>What is the first thing that comes to mind whe...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>What career advice would you give to someone w...</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>Do other countries have as much coverage of UF...</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3779 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  rating\n",
       "0     What's something nice you like to do just to b...    4.40\n",
       "1     So what is the best headphones for people who ...    4.50\n",
       "2     How do you go on knowing a loved one only has ...    4.80\n",
       "3     You’ve been dropped to the year 1800 with all ...    1.40\n",
       "4     Stuck in bad habits for years and I realized i...    5.00\n",
       "...                                                 ...     ...\n",
       "3774  Is this normal? Should I say something? From a...    4.50\n",
       "3775  If Gender is how you feel, and not what your b...    4.25\n",
       "3776  What is the first thing that comes to mind whe...    4.00\n",
       "3777  What career advice would you give to someone w...    4.75\n",
       "3778  Do other countries have as much coverage of UF...    4.75\n",
       "\n",
       "[3779 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = []\n",
    "score_list = []\n",
    "for index, row in data.iterrows():\n",
    "    s = 0\n",
    "    cnt = 0\n",
    "    for score in train.loc[row[\"question_id\"] == train[\"id\"]][\"rating\"]:\n",
    "        if not np.isnan(score):\n",
    "            s += score\n",
    "            cnt += 1\n",
    "    if cnt != 0:\n",
    "        text_list.append(row[\"question_text\"] + \" \" + row[\"reply_text\"])\n",
    "        score_list.append(s / cnt)\n",
    "df = pd.DataFrame({\"text\": text_list, \"rating\": score_list}) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_q</th>\n",
       "      <th>text_a</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's something nice you like to do just to b...</td>\n",
       "      <td>Give compliments. It’s extremely easy to do an...</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So what is the best headphones for people who ...</td>\n",
       "      <td>I prefer Raycon Performance Ear Buds. They are...</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you go on knowing a loved one only has ...</td>\n",
       "      <td>Make it as memorable as the rest of your time ...</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You’ve been dropped to the year 1800 with all ...</td>\n",
       "      <td>They're gonna burn me at the stake for being a...</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stuck in bad habits for years and I realized i...</td>\n",
       "      <td>Using new environments as a way to create heal...</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>Is this normal? Should I say something?</td>\n",
       "      <td>From a guys perspective, this is definitely NO...</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>If Gender is how you feel, and not what your b...</td>\n",
       "      <td>Gender is how you feel. Sometimes it matches w...</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>What is the first thing that comes to mind whe...</td>\n",
       "      <td>Austin, we have a problem. Just watched an epi...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>What career advice would you give to someone w...</td>\n",
       "      <td>Work anywhere while you build qualifications a...</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>Do other countries have as much coverage of UF...</td>\n",
       "      <td>In Mexico, there’s barely any news if any but ...</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3779 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text_q  \\\n",
       "0     What's something nice you like to do just to b...   \n",
       "1     So what is the best headphones for people who ...   \n",
       "2     How do you go on knowing a loved one only has ...   \n",
       "3     You’ve been dropped to the year 1800 with all ...   \n",
       "4     Stuck in bad habits for years and I realized i...   \n",
       "...                                                 ...   \n",
       "3774            Is this normal? Should I say something?   \n",
       "3775  If Gender is how you feel, and not what your b...   \n",
       "3776  What is the first thing that comes to mind whe...   \n",
       "3777  What career advice would you give to someone w...   \n",
       "3778  Do other countries have as much coverage of UF...   \n",
       "\n",
       "                                                 text_a  rating  \n",
       "0     Give compliments. It’s extremely easy to do an...    4.40  \n",
       "1     I prefer Raycon Performance Ear Buds. They are...    4.50  \n",
       "2     Make it as memorable as the rest of your time ...    4.80  \n",
       "3     They're gonna burn me at the stake for being a...    1.40  \n",
       "4     Using new environments as a way to create heal...    5.00  \n",
       "...                                                 ...     ...  \n",
       "3774  From a guys perspective, this is definitely NO...    4.50  \n",
       "3775  Gender is how you feel. Sometimes it matches w...    4.25  \n",
       "3776  Austin, we have a problem. Just watched an epi...    4.00  \n",
       "3777  Work anywhere while you build qualifications a...    4.75  \n",
       "3778  In Mexico, there’s barely any news if any but ...    4.75  \n",
       "\n",
       "[3779 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list_q = []\n",
    "text_list_a = []\n",
    "score_list = []\n",
    "for index, row in data.iterrows():\n",
    "    s = 0\n",
    "    cnt = 0\n",
    "    for score in train.loc[row[\"question_id\"] == train[\"id\"]][\"rating\"]:\n",
    "        if not np.isnan(score):\n",
    "            s += score\n",
    "            cnt += 1\n",
    "    if cnt != 0:\n",
    "        text_list_q.append(row[\"question_text\"])\n",
    "        text_list_a.append(row[\"reply_text\"])\n",
    "        score_list.append(s / cnt)\n",
    "df2 = pd.DataFrame({\"text_q\": text_list_q, \"text_a\": text_list_a, \"rating\": score_list}) \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ELI5: How do analog signals (magnetic tape in ...</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People who carry around a towel with them, why...</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Architects, How Much Math Is Required In Yo...</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What’s touristy thing have you been so excited...</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do you know about Palestine? They have a ...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>What simply skill did you learn recently do yo...</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>What was the most passive-aggressive gift you ...</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>What is a weird fact that no one really thinks...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>What did you forgive a partner for that you wi...</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>What was your most unexplainable and bizarre e...</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>811 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  rating\n",
       "0    ELI5: How do analog signals (magnetic tape in ...    4.60\n",
       "1    People who carry around a towel with them, why...    1.40\n",
       "2    To Architects, How Much Math Is Required In Yo...    3.20\n",
       "3    What’s touristy thing have you been so excited...    5.00\n",
       "4    What do you know about Palestine? They have a ...    3.40\n",
       "..                                                 ...     ...\n",
       "806  What simply skill did you learn recently do yo...    2.75\n",
       "807  What was the most passive-aggressive gift you ...    4.50\n",
       "808  What is a weird fact that no one really thinks...    3.75\n",
       "809  What did you forgive a partner for that you wi...    4.75\n",
       "810  What was your most unexplainable and bizarre e...    4.25\n",
       "\n",
       "[811 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_csv(\"si630w22-hw3-dev.csv\")\n",
    "text_list = []\n",
    "score_list = []\n",
    "for index, row in data.iterrows():\n",
    "    s = 0\n",
    "    cnt = 0\n",
    "    for score in dev.loc[row[\"question_id\"] == dev[\"id\"]][\"rating\"]:\n",
    "        if not np.isnan(score):\n",
    "            s += score\n",
    "            cnt += 1\n",
    "    if cnt != 0:\n",
    "        text_list.append(row[\"question_text\"] + \" \" + row[\"reply_text\"])\n",
    "        score_list.append(s / cnt)\n",
    "df_dev = pd.DataFrame({\"text\": text_list, \"rating\": score_list}) \n",
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_q</th>\n",
       "      <th>text_a</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ELI5: How do analog signals (magnetic tape in ...</td>\n",
       "      <td>It’s varying levels. Think of a photograph on ...</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People who carry around a towel with them, why?</td>\n",
       "      <td>So everyone knows \"that's a frood who knows wh...</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Architects, How Much Math Is Required In Yo...</td>\n",
       "      <td>For Context: I'm a Year 11 Student (16 y.o) ha...</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What’s touristy thing have you been so excited...</td>\n",
       "      <td>Not a *total* let down, but they don't let you...</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do you know about Palestine?</td>\n",
       "      <td>They have a just cause, but totally wrong appr...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>What simply skill did you learn recently do yo...</td>\n",
       "      <td>Grammar and spelling is quite SIMPLE. Would re...</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>What was the most passive-aggressive gift you ...</td>\n",
       "      <td>My sister in law, not me. Our father in law an...</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>What is a weird fact that no one really thinks...</td>\n",
       "      <td>Concept of souls and reincarnation. The has to...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>What did you forgive a partner for that you wi...</td>\n",
       "      <td>My hubby got taken in by a Russian bride scam ...</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>What was your most unexplainable and bizarre e...</td>\n",
       "      <td>Doing my homework at the desk when I was like ...</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>811 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text_q  \\\n",
       "0    ELI5: How do analog signals (magnetic tape in ...   \n",
       "1      People who carry around a towel with them, why?   \n",
       "2    To Architects, How Much Math Is Required In Yo...   \n",
       "3    What’s touristy thing have you been so excited...   \n",
       "4                    What do you know about Palestine?   \n",
       "..                                                 ...   \n",
       "806  What simply skill did you learn recently do yo...   \n",
       "807  What was the most passive-aggressive gift you ...   \n",
       "808  What is a weird fact that no one really thinks...   \n",
       "809  What did you forgive a partner for that you wi...   \n",
       "810  What was your most unexplainable and bizarre e...   \n",
       "\n",
       "                                                text_a  rating  \n",
       "0    It’s varying levels. Think of a photograph on ...    4.60  \n",
       "1    So everyone knows \"that's a frood who knows wh...    1.40  \n",
       "2    For Context: I'm a Year 11 Student (16 y.o) ha...    3.20  \n",
       "3    Not a *total* let down, but they don't let you...    5.00  \n",
       "4    They have a just cause, but totally wrong appr...    3.40  \n",
       "..                                                 ...     ...  \n",
       "806  Grammar and spelling is quite SIMPLE. Would re...    2.75  \n",
       "807  My sister in law, not me. Our father in law an...    4.50  \n",
       "808  Concept of souls and reincarnation. The has to...    3.75  \n",
       "809  My hubby got taken in by a Russian bride scam ...    4.75  \n",
       "810  Doing my homework at the desk when I was like ...    4.25  \n",
       "\n",
       "[811 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list_q = []\n",
    "text_list_a = []\n",
    "score_list = []\n",
    "for index, row in data.iterrows():\n",
    "    s = 0\n",
    "    cnt = 0\n",
    "    for score in dev.loc[row[\"question_id\"] == dev[\"id\"]][\"rating\"]:\n",
    "        if not np.isnan(score):\n",
    "            s += score\n",
    "            cnt += 1\n",
    "    if cnt != 0:\n",
    "        text_list_q.append(row[\"question_text\"])\n",
    "        text_list_a.append(row[\"reply_text\"])\n",
    "        score_list.append(s / cnt)\n",
    "df_dev2 = pd.DataFrame({\"text_q\": text_list_q, \"text_a\": text_list_a, \"rating\": score_list}) \n",
    "df_dev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I be insecure of my penis size?Truth is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do you guys lay out for monthly AND weekly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What business could you realistic start with $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has anyone ever actually been in a classic hor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you could go back in time and change one th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>What is something you did but never told anyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>How will human civilization end?At it's own hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>How do you freshen up in the bathroom every mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>What was the biggest mistake you've ever made,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>What is the worst present you have ever receiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    Should I be insecure of my penis size?Truth is...\n",
       "1    how do you guys lay out for monthly AND weekly...\n",
       "2    What business could you realistic start with $...\n",
       "3    Has anyone ever actually been in a classic hor...\n",
       "4    If you could go back in time and change one th...\n",
       "..                                                 ...\n",
       "805  What is something you did but never told anyon...\n",
       "806   How will human civilization end?At it's own hand\n",
       "807  How do you freshen up in the bathroom every mo...\n",
       "808  What was the biggest mistake you've ever made,...\n",
       "809  What is the worst present you have ever receiv...\n",
       "\n",
       "[810 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"si630w22-hw3-test.public.csv\")\n",
    "text_list = []\n",
    "for index, row in data.iterrows():\n",
    "    if len(test.loc[row[\"question_id\"] == test[\"id\"]]) != 0:\n",
    "        text_list.append(row[\"question_text\"] + row[\"reply_text\"])\n",
    "df_test = pd.DataFrame({\"text\": text_list}) \n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_q</th>\n",
       "      <th>text_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_n2ooiu</td>\n",
       "      <td>Should I be insecure of my penis size?</td>\n",
       "      <td>Truth is, the vast majority of women can't org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_n2to6d</td>\n",
       "      <td>how do you guys lay out for monthly AND weekly...</td>\n",
       "      <td>I typically only do weekly pages, but the mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_n2xuk5</td>\n",
       "      <td>What business could you realistic start with $...</td>\n",
       "      <td>Auto detailing, lawn care my top 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_n2xrc5</td>\n",
       "      <td>Has anyone ever actually been in a classic hor...</td>\n",
       "      <td>Idk if this counts, but my parents told me tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_n2yhgh</td>\n",
       "      <td>If you could go back in time and change one th...</td>\n",
       "      <td>I'd tell my 12 year old self about what being ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>t3_n9w5ix</td>\n",
       "      <td>What is something you did but never told anyone?</td>\n",
       "      <td>Pretty tame, but: When my older brother was a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>t3_nf3a7q</td>\n",
       "      <td>How will human civilization end?</td>\n",
       "      <td>At it's own hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>t3_npg62d</td>\n",
       "      <td>How do you freshen up in the bathroom every mo...</td>\n",
       "      <td>I take a look in the mirror and say \"what's up\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>t3_nctt1a</td>\n",
       "      <td>What was the biggest mistake you've ever made,...</td>\n",
       "      <td>I volunteered to DJ at my high school's basket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>t3_nojmhe</td>\n",
       "      <td>What is the worst present you have ever receiv...</td>\n",
       "      <td>NOPE I have a different answer! My father's le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                             text_q  \\\n",
       "0    t3_n2ooiu             Should I be insecure of my penis size?   \n",
       "1    t3_n2to6d  how do you guys lay out for monthly AND weekly...   \n",
       "2    t3_n2xuk5  What business could you realistic start with $...   \n",
       "3    t3_n2xrc5  Has anyone ever actually been in a classic hor...   \n",
       "4    t3_n2yhgh  If you could go back in time and change one th...   \n",
       "..         ...                                                ...   \n",
       "805  t3_n9w5ix   What is something you did but never told anyone?   \n",
       "806  t3_nf3a7q                   How will human civilization end?   \n",
       "807  t3_npg62d  How do you freshen up in the bathroom every mo...   \n",
       "808  t3_nctt1a  What was the biggest mistake you've ever made,...   \n",
       "809  t3_nojmhe  What is the worst present you have ever receiv...   \n",
       "\n",
       "                                                text_a  \n",
       "0    Truth is, the vast majority of women can't org...  \n",
       "1    I typically only do weekly pages, but the mont...  \n",
       "2                   Auto detailing, lawn care my top 2  \n",
       "3    Idk if this counts, but my parents told me tha...  \n",
       "4    I'd tell my 12 year old self about what being ...  \n",
       "..                                                 ...  \n",
       "805  Pretty tame, but: When my older brother was a ...  \n",
       "806                                   At it's own hand  \n",
       "807    I take a look in the mirror and say \"what's up\"  \n",
       "808  I volunteered to DJ at my high school's basket...  \n",
       "809  NOPE I have a different answer! My father's le...  \n",
       "\n",
       "[810 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"si630w22-hw3-test.public.csv\")\n",
    "text_list_q = []\n",
    "text_list_a = []\n",
    "id_list = []\n",
    "for index, row in data.iterrows():\n",
    "    if len(test.loc[row[\"question_id\"] == test[\"id\"]]) != 0:\n",
    "        text_list_q.append(row[\"question_text\"])\n",
    "        text_list_a.append(row[\"reply_text\"])\n",
    "        id_list.append(row[\"question_id\"])\n",
    "df_test2 = pd.DataFrame({\"id\": id_list, \"text_q\": text_list_q, \"text_a\": text_list_a}) \n",
    "df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/vocab.txt from cache at /home/sylva/.cache/huggingface/transformers/49c302ee103bf6737d0877cfbd658563cf4bbc4b7914363ca419ce8a3d8a4c51.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/special_tokens_map.json from cache at /home/sylva/.cache/huggingface/transformers/1e5909e4dfaa904617797ed35a6105a23daa56cbefca48fef329f772584699fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer_config.json from cache at /home/sylva/.cache/huggingface/transformers/29039dfe8c131360348e9f5ebecd464478cec7576c9af532b55ddcf9d4ec8d1e.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/pytorch_model.bin from cache at /home/sylva/.cache/huggingface/transformers/b774244369e464de2c660477b70bae7c3223fa7250aa1c8fc0b0f037ed58418a.087808d17814e241e9352c5ce0fea1a7d05e5b0f020d44b42b5f05922e96c923\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 3779\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='522' max='1419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 522/1419 01:21 < 02:21, 6.36 it/s, Epoch 1.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.179000</td>\n",
       "      <td>0.759764</td>\n",
       "      <td>0.232935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 811\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to output_A3/checkpoint-500\n",
      "Configuration saved in output_A3/checkpoint-500/config.json\n",
      "Model weights saved in output_A3/checkpoint-500/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ff5e600a9da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Train pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 if (\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define pretrained tokenizer and model\n",
    "model_name = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# ----- 1. Preprocess data -----#\n",
    "# Preprocess data\n",
    "# X_train = list(df[\"text\"])\n",
    "# y_train = list(df[\"rating\"])\n",
    "# X_val = list(df_dev[\"text\"])\n",
    "# y_val = list(df_dev[\"rating\"])\n",
    "\n",
    "# X = list(df[\"text\"])\n",
    "# y = list(df[\"rating\"])\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "# X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "X_train_tokenized = tokenizer(list(df2[\"text_q\"]), list(df2[\"text_a\"]), padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(list(df_dev2[\"text_q\"]), list(df_dev2[\"text_a\"]), padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     pred, labels = p\n",
    "#     pred = np.argmax(pred, axis=1)\n",
    "#     accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "#     recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n",
    "#     precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n",
    "#     f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "#     return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     pred, labels = p\n",
    "#     pred = pred.reshape((len(pred),))\n",
    "#     return {\"MSE\": np.sum((pred - labels) ** 2 / len(pred))}\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = pred.reshape((len(pred),))\n",
    "    return {\"r\": np.corrcoef(pred, labels)[0,1]}\n",
    "\n",
    "\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output_A3\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train pre-trained model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "loading configuration file output_A3/checkpoint-1000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/MiniLM-L12-H384-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output_A3/checkpoint-1000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at output_A3/checkpoint-1000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 810\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- 3. Predict -----#\n",
    "# Load test data\n",
    "# X_test = list(df_test[\"text\"])\n",
    "# X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "X_test_tokenized = tokenizer(list(df_test2[\"text_q\"]), list(df_test2[\"text_a\"]),  padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"output_A3/checkpoint-1000\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "# y_pred = np.argmax(raw_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.5447497],\n",
       "       [4.241802 ],\n",
       "       [3.4033468],\n",
       "       [4.541659 ],\n",
       "       [3.403532 ],\n",
       "       [3.4117258],\n",
       "       [4.5453944],\n",
       "       [3.4034283],\n",
       "       [3.7401311],\n",
       "       [3.403266 ],\n",
       "       [4.53986  ],\n",
       "       [3.4126484],\n",
       "       [3.4033992],\n",
       "       [3.4030669],\n",
       "       [4.543202 ],\n",
       "       [3.4033773],\n",
       "       [3.403411 ],\n",
       "       [4.544922 ],\n",
       "       [3.403387 ],\n",
       "       [4.5415964],\n",
       "       [3.4034464],\n",
       "       [4.532633 ],\n",
       "       [4.5449224],\n",
       "       [3.4035335],\n",
       "       [3.403495 ],\n",
       "       [4.54333  ],\n",
       "       [3.4034963],\n",
       "       [4.510979 ],\n",
       "       [3.766831 ],\n",
       "       [4.5444655],\n",
       "       [4.473453 ],\n",
       "       [4.264784 ],\n",
       "       [3.4034588],\n",
       "       [4.542552 ],\n",
       "       [3.4034162],\n",
       "       [4.545183 ],\n",
       "       [3.4033983],\n",
       "       [3.4034042],\n",
       "       [4.545101 ],\n",
       "       [3.4032319],\n",
       "       [3.4035265],\n",
       "       [3.403417 ],\n",
       "       [3.4033425],\n",
       "       [3.4033422],\n",
       "       [4.5391626],\n",
       "       [3.4033792],\n",
       "       [4.5434394],\n",
       "       [3.403455 ],\n",
       "       [3.4033735],\n",
       "       [4.515664 ],\n",
       "       [3.403498 ],\n",
       "       [3.4035206],\n",
       "       [3.403495 ],\n",
       "       [3.4033394],\n",
       "       [3.403451 ],\n",
       "       [4.5452304],\n",
       "       [3.4035585],\n",
       "       [4.5446644],\n",
       "       [3.403136 ],\n",
       "       [3.403461 ],\n",
       "       [3.4034069],\n",
       "       [3.403408 ],\n",
       "       [3.403451 ],\n",
       "       [3.4034507],\n",
       "       [4.5410757],\n",
       "       [3.4034557],\n",
       "       [3.4033587],\n",
       "       [4.544905 ],\n",
       "       [4.5402565],\n",
       "       [3.4034538],\n",
       "       [3.403413 ],\n",
       "       [3.4034705],\n",
       "       [4.5434165],\n",
       "       [4.51659  ],\n",
       "       [4.4796915],\n",
       "       [3.4162817],\n",
       "       [3.4034774],\n",
       "       [3.4033794],\n",
       "       [3.4033499],\n",
       "       [4.5448627],\n",
       "       [3.4034917],\n",
       "       [4.5450964],\n",
       "       [3.4033656],\n",
       "       [3.4034328],\n",
       "       [4.544911 ],\n",
       "       [4.545024 ],\n",
       "       [3.403316 ],\n",
       "       [3.4034379],\n",
       "       [3.4252307],\n",
       "       [4.5332036],\n",
       "       [4.2792263],\n",
       "       [4.544229 ],\n",
       "       [4.5428376],\n",
       "       [3.4034646],\n",
       "       [4.542884 ],\n",
       "       [4.544928 ],\n",
       "       [3.4032032],\n",
       "       [4.5390825],\n",
       "       [3.403525 ],\n",
       "       [4.539764 ],\n",
       "       [3.403416 ],\n",
       "       [3.4034543],\n",
       "       [3.4034827],\n",
       "       [3.403484 ],\n",
       "       [3.403385 ],\n",
       "       [4.544535 ],\n",
       "       [4.5451074],\n",
       "       [3.4032938],\n",
       "       [3.4034407],\n",
       "       [4.544839 ],\n",
       "       [3.4981563],\n",
       "       [3.4072413],\n",
       "       [4.544859 ],\n",
       "       [4.5426927],\n",
       "       [3.4042714],\n",
       "       [4.5449524],\n",
       "       [4.539438 ],\n",
       "       [3.4034388],\n",
       "       [3.4051692],\n",
       "       [3.4033406],\n",
       "       [3.403168 ],\n",
       "       [3.4032042],\n",
       "       [4.5451307],\n",
       "       [4.5430655],\n",
       "       [4.5448895],\n",
       "       [4.5409846],\n",
       "       [3.4034586],\n",
       "       [3.4034681],\n",
       "       [3.4034889],\n",
       "       [3.4034293],\n",
       "       [3.4034944],\n",
       "       [3.4034467],\n",
       "       [3.403535 ],\n",
       "       [3.4029715],\n",
       "       [3.4050539],\n",
       "       [3.4034321],\n",
       "       [3.4035444],\n",
       "       [4.5232534],\n",
       "       [3.403328 ],\n",
       "       [3.4033482],\n",
       "       [3.4034777],\n",
       "       [3.4032748],\n",
       "       [4.544809 ],\n",
       "       [3.403373 ],\n",
       "       [4.5447636],\n",
       "       [4.5292363],\n",
       "       [3.4034042],\n",
       "       [3.40339  ],\n",
       "       [3.4034615],\n",
       "       [4.5248837],\n",
       "       [4.5399384],\n",
       "       [4.520772 ],\n",
       "       [3.403425 ],\n",
       "       [4.5434155],\n",
       "       [4.5451393],\n",
       "       [4.544415 ],\n",
       "       [3.4032955],\n",
       "       [3.4034686],\n",
       "       [3.4033399],\n",
       "       [3.4036329],\n",
       "       [4.544903 ],\n",
       "       [3.4035156],\n",
       "       [3.4034946],\n",
       "       [3.4032848],\n",
       "       [4.545165 ],\n",
       "       [3.4032462],\n",
       "       [3.403257 ],\n",
       "       [4.5450845],\n",
       "       [3.4034448],\n",
       "       [3.4034262],\n",
       "       [3.4033744],\n",
       "       [3.403465 ],\n",
       "       [4.532235 ],\n",
       "       [3.526389 ],\n",
       "       [3.4034002],\n",
       "       [4.54504  ],\n",
       "       [4.541309 ],\n",
       "       [3.4033594],\n",
       "       [3.4033122],\n",
       "       [3.4034495],\n",
       "       [4.5446277],\n",
       "       [3.4033804],\n",
       "       [3.4219608],\n",
       "       [4.5354753],\n",
       "       [3.403412 ],\n",
       "       [3.7925446],\n",
       "       [3.4034138],\n",
       "       [4.5453634],\n",
       "       [4.544549 ],\n",
       "       [3.4961288],\n",
       "       [3.4033575],\n",
       "       [4.5451355],\n",
       "       [3.4034443],\n",
       "       [4.5451035],\n",
       "       [4.5446763],\n",
       "       [3.405848 ],\n",
       "       [4.5448737],\n",
       "       [3.4034727],\n",
       "       [3.403472 ],\n",
       "       [4.5449224],\n",
       "       [4.5446763],\n",
       "       [4.5448246],\n",
       "       [4.5431995],\n",
       "       [4.541855 ],\n",
       "       [3.403388 ],\n",
       "       [3.403366 ],\n",
       "       [4.544964 ],\n",
       "       [3.4034655],\n",
       "       [3.403457 ],\n",
       "       [4.5227723],\n",
       "       [4.5447993],\n",
       "       [4.4321594],\n",
       "       [4.5391555],\n",
       "       [4.543372 ],\n",
       "       [3.4034584],\n",
       "       [4.5445046],\n",
       "       [4.542382 ],\n",
       "       [3.4034507],\n",
       "       [3.4034646],\n",
       "       [3.4033456],\n",
       "       [3.4035552],\n",
       "       [3.4053574],\n",
       "       [3.403385 ],\n",
       "       [3.403428 ],\n",
       "       [3.403414 ],\n",
       "       [3.4033828],\n",
       "       [3.4034803],\n",
       "       [4.54492  ],\n",
       "       [3.4035728],\n",
       "       [3.403393 ],\n",
       "       [3.4034276],\n",
       "       [3.4034646],\n",
       "       [4.544362 ],\n",
       "       [3.5537062],\n",
       "       [3.4034696],\n",
       "       [3.403393 ],\n",
       "       [3.4034379],\n",
       "       [3.403446 ],\n",
       "       [4.5449843],\n",
       "       [3.5922952],\n",
       "       [4.5430984],\n",
       "       [3.403418 ],\n",
       "       [4.5414534],\n",
       "       [4.545112 ],\n",
       "       [4.543893 ],\n",
       "       [4.542162 ],\n",
       "       [3.4035127],\n",
       "       [3.4034858],\n",
       "       [4.5317674],\n",
       "       [3.4034233],\n",
       "       [3.403476 ],\n",
       "       [3.4033587],\n",
       "       [4.545012 ],\n",
       "       [4.543508 ],\n",
       "       [3.4031653],\n",
       "       [4.5407724],\n",
       "       [4.4194503],\n",
       "       [3.403413 ],\n",
       "       [3.40329  ],\n",
       "       [3.403483 ],\n",
       "       [3.4034812],\n",
       "       [4.5352426],\n",
       "       [4.493189 ],\n",
       "       [3.4032993],\n",
       "       [3.4034393],\n",
       "       [3.403506 ],\n",
       "       [3.4033318],\n",
       "       [3.4034734],\n",
       "       [3.4034572],\n",
       "       [3.4033952],\n",
       "       [3.4033117],\n",
       "       [3.4034898],\n",
       "       [3.8999126],\n",
       "       [3.4033875],\n",
       "       [4.5445185],\n",
       "       [4.536419 ],\n",
       "       [4.5294065],\n",
       "       [3.4034064],\n",
       "       [3.403357 ],\n",
       "       [3.4033906],\n",
       "       [4.5451245],\n",
       "       [3.4033568],\n",
       "       [3.4035294],\n",
       "       [4.545013 ],\n",
       "       [4.5349736],\n",
       "       [3.4034333],\n",
       "       [4.5412803],\n",
       "       [4.5431023],\n",
       "       [3.4035065],\n",
       "       [4.5436296],\n",
       "       [3.4034212],\n",
       "       [4.5387354],\n",
       "       [4.5446134],\n",
       "       [3.4397266],\n",
       "       [4.5414248],\n",
       "       [3.403483 ],\n",
       "       [3.403337 ],\n",
       "       [3.403148 ],\n",
       "       [3.4032624],\n",
       "       [3.403362 ],\n",
       "       [3.4033515],\n",
       "       [4.5434227],\n",
       "       [3.4033887],\n",
       "       [3.4032772],\n",
       "       [4.541118 ],\n",
       "       [4.5440307],\n",
       "       [4.535702 ],\n",
       "       [3.4034379],\n",
       "       [3.4091349],\n",
       "       [4.5400987],\n",
       "       [3.4033828],\n",
       "       [3.405764 ],\n",
       "       [3.4033408],\n",
       "       [3.490096 ],\n",
       "       [3.4034047],\n",
       "       [4.5450935],\n",
       "       [3.4031875],\n",
       "       [3.4031816],\n",
       "       [4.536743 ],\n",
       "       [3.4052365],\n",
       "       [4.5449076],\n",
       "       [3.4033594],\n",
       "       [3.4034436],\n",
       "       [3.4033937],\n",
       "       [3.4033756],\n",
       "       [4.543054 ],\n",
       "       [3.7679417],\n",
       "       [3.4036682],\n",
       "       [4.5449405],\n",
       "       [4.5450034],\n",
       "       [4.539141 ],\n",
       "       [4.5402355],\n",
       "       [3.4058897],\n",
       "       [4.5322328],\n",
       "       [4.5450616],\n",
       "       [4.5451427],\n",
       "       [4.4488554],\n",
       "       [3.4033287],\n",
       "       [3.4122114],\n",
       "       [4.5389833],\n",
       "       [4.5237823],\n",
       "       [4.5452538],\n",
       "       [3.4033384],\n",
       "       [3.4034793],\n",
       "       [4.5446978],\n",
       "       [3.4034123],\n",
       "       [3.4031987],\n",
       "       [4.5346456],\n",
       "       [4.5446806],\n",
       "       [4.544145 ],\n",
       "       [3.4033773],\n",
       "       [3.4034011],\n",
       "       [4.276442 ],\n",
       "       [3.4032829],\n",
       "       [3.403394 ],\n",
       "       [4.5385385],\n",
       "       [3.8594835],\n",
       "       [3.4034612],\n",
       "       [3.4035344],\n",
       "       [3.4033449],\n",
       "       [4.543372 ],\n",
       "       [4.543587 ],\n",
       "       [3.4034326],\n",
       "       [3.4033573],\n",
       "       [4.5452867],\n",
       "       [3.40339  ],\n",
       "       [3.403157 ],\n",
       "       [4.5403795],\n",
       "       [3.4033678],\n",
       "       [3.4892118],\n",
       "       [3.403412 ],\n",
       "       [3.4034297],\n",
       "       [3.4033873],\n",
       "       [4.537148 ],\n",
       "       [4.5449286],\n",
       "       [4.5285425],\n",
       "       [4.5425725],\n",
       "       [4.521996 ],\n",
       "       [3.6139438],\n",
       "       [4.5421867],\n",
       "       [4.539628 ],\n",
       "       [3.4035118],\n",
       "       [4.5437064],\n",
       "       [3.4034553],\n",
       "       [4.504407 ],\n",
       "       [3.4607651],\n",
       "       [3.4070485],\n",
       "       [4.4842877],\n",
       "       [3.4034398],\n",
       "       [3.4033945],\n",
       "       [3.403525 ],\n",
       "       [3.403497 ],\n",
       "       [3.4045086],\n",
       "       [4.5399404],\n",
       "       [3.4035425],\n",
       "       [4.5443387],\n",
       "       [3.403385 ],\n",
       "       [3.4032876],\n",
       "       [4.5451846],\n",
       "       [3.403392 ],\n",
       "       [3.4134102],\n",
       "       [3.4034576],\n",
       "       [3.4034474],\n",
       "       [3.4033775],\n",
       "       [3.4034224],\n",
       "       [3.403493 ],\n",
       "       [4.544678 ],\n",
       "       [3.403569 ],\n",
       "       [3.4028032],\n",
       "       [3.403451 ],\n",
       "       [3.4034073],\n",
       "       [3.4034634],\n",
       "       [4.5444713],\n",
       "       [3.403536 ],\n",
       "       [3.4032986],\n",
       "       [3.4034686],\n",
       "       [3.4034407],\n",
       "       [3.4033291],\n",
       "       [3.4034023],\n",
       "       [4.544954 ],\n",
       "       [4.5399036],\n",
       "       [4.5071354],\n",
       "       [4.530174 ],\n",
       "       [3.4034584],\n",
       "       [3.403313 ],\n",
       "       [3.4906583],\n",
       "       [3.4033923],\n",
       "       [3.4641805],\n",
       "       [3.403478 ],\n",
       "       [3.4045637],\n",
       "       [4.5450916],\n",
       "       [3.4033458],\n",
       "       [3.4034307],\n",
       "       [4.5448213],\n",
       "       [3.4033535],\n",
       "       [4.528367 ],\n",
       "       [4.5433636],\n",
       "       [4.537828 ],\n",
       "       [3.4034827],\n",
       "       [4.4953537],\n",
       "       [3.4031823],\n",
       "       [4.543713 ],\n",
       "       [4.5446754],\n",
       "       [4.5414515],\n",
       "       [3.403418 ],\n",
       "       [4.514267 ],\n",
       "       [4.4558372],\n",
       "       [4.5441203],\n",
       "       [3.403461 ],\n",
       "       [4.540914 ],\n",
       "       [3.4034894],\n",
       "       [4.536599 ],\n",
       "       [4.5450435],\n",
       "       [4.5402026],\n",
       "       [3.4034293],\n",
       "       [3.404664 ],\n",
       "       [4.544409 ],\n",
       "       [4.5449114],\n",
       "       [3.403405 ],\n",
       "       [4.5411987],\n",
       "       [4.5451508],\n",
       "       [3.4034457],\n",
       "       [4.5434985],\n",
       "       [3.4034076],\n",
       "       [3.4033053],\n",
       "       [3.4033828],\n",
       "       [3.4034362],\n",
       "       [3.4034865],\n",
       "       [4.5179915],\n",
       "       [4.542904 ],\n",
       "       [3.4033499],\n",
       "       [3.403387 ],\n",
       "       [4.5379524],\n",
       "       [4.5415735],\n",
       "       [3.403358 ],\n",
       "       [3.5292902],\n",
       "       [4.3682976],\n",
       "       [3.548171 ],\n",
       "       [3.4034178],\n",
       "       [4.529837 ],\n",
       "       [4.538261 ],\n",
       "       [3.4033442],\n",
       "       [3.4033678],\n",
       "       [4.544216 ],\n",
       "       [3.4033368],\n",
       "       [3.403268 ],\n",
       "       [4.5451927],\n",
       "       [4.5450525],\n",
       "       [3.4033382],\n",
       "       [3.4027567],\n",
       "       [3.4033644],\n",
       "       [4.544931 ],\n",
       "       [3.4033926],\n",
       "       [3.4034445],\n",
       "       [3.403477 ],\n",
       "       [3.4034553],\n",
       "       [4.5452676],\n",
       "       [3.4034822],\n",
       "       [4.495639 ],\n",
       "       [3.40342  ],\n",
       "       [4.541565 ],\n",
       "       [4.540039 ],\n",
       "       [3.4031365],\n",
       "       [4.544843 ],\n",
       "       [3.4034271],\n",
       "       [4.0218678],\n",
       "       [4.5400414],\n",
       "       [4.536402 ],\n",
       "       [4.5135436],\n",
       "       [3.403363 ],\n",
       "       [4.544986 ],\n",
       "       [3.4033859],\n",
       "       [3.403511 ],\n",
       "       [3.4034164],\n",
       "       [3.403379 ],\n",
       "       [3.4033327],\n",
       "       [3.4034638],\n",
       "       [3.4034333],\n",
       "       [3.403457 ],\n",
       "       [4.543953 ],\n",
       "       [4.5444956],\n",
       "       [4.544983 ],\n",
       "       [3.403341 ],\n",
       "       [3.4034204],\n",
       "       [3.403389 ],\n",
       "       [4.528291 ],\n",
       "       [4.540065 ],\n",
       "       [4.5411024],\n",
       "       [4.5445986],\n",
       "       [3.4034672],\n",
       "       [3.4033601],\n",
       "       [4.545063 ],\n",
       "       [3.6537306],\n",
       "       [4.5451965],\n",
       "       [4.545046 ],\n",
       "       [4.54494  ],\n",
       "       [4.541012 ],\n",
       "       [4.54504  ],\n",
       "       [3.403508 ],\n",
       "       [3.4034114],\n",
       "       [4.4940166],\n",
       "       [4.544991 ],\n",
       "       [4.5452576],\n",
       "       [3.4033365],\n",
       "       [4.5446467],\n",
       "       [4.544375 ],\n",
       "       [4.5381536],\n",
       "       [4.544778 ],\n",
       "       [4.544694 ],\n",
       "       [4.5441456],\n",
       "       [3.4034398],\n",
       "       [3.403421 ],\n",
       "       [3.4033668],\n",
       "       [3.403518 ],\n",
       "       [3.4034388],\n",
       "       [4.5448966],\n",
       "       [3.4034407],\n",
       "       [3.403469 ],\n",
       "       [4.5447097],\n",
       "       [4.5445085],\n",
       "       [3.4033465],\n",
       "       [3.4032621],\n",
       "       [4.5377307],\n",
       "       [4.5414   ],\n",
       "       [4.37301  ],\n",
       "       [3.403332 ],\n",
       "       [4.5452566],\n",
       "       [4.544227 ],\n",
       "       [4.5010448],\n",
       "       [3.4033878],\n",
       "       [3.4032636],\n",
       "       [4.5431657],\n",
       "       [4.5447555],\n",
       "       [3.4032447],\n",
       "       [3.4032981],\n",
       "       [4.406675 ],\n",
       "       [4.545026 ],\n",
       "       [4.5440288],\n",
       "       [3.5465176],\n",
       "       [4.544605 ],\n",
       "       [3.4034052],\n",
       "       [4.5448914],\n",
       "       [4.5450673],\n",
       "       [3.4034808],\n",
       "       [3.4034321],\n",
       "       [4.538787 ],\n",
       "       [3.4033756],\n",
       "       [3.4034727],\n",
       "       [4.5448356],\n",
       "       [3.7346153],\n",
       "       [4.5402226],\n",
       "       [3.4033835],\n",
       "       [4.5416384],\n",
       "       [3.4031742],\n",
       "       [4.525512 ],\n",
       "       [4.541131 ],\n",
       "       [4.5445433],\n",
       "       [3.4034984],\n",
       "       [3.4087284],\n",
       "       [3.4034336],\n",
       "       [3.4034853],\n",
       "       [4.5446005],\n",
       "       [3.4262977],\n",
       "       [4.5446563],\n",
       "       [4.5445805],\n",
       "       [3.4033368],\n",
       "       [4.5449443],\n",
       "       [3.4034243],\n",
       "       [4.112462 ],\n",
       "       [4.545301 ],\n",
       "       [3.402962 ],\n",
       "       [4.5449123],\n",
       "       [3.4030774],\n",
       "       [4.5374846],\n",
       "       [3.4034448],\n",
       "       [4.5427337],\n",
       "       [4.544829 ],\n",
       "       [4.544271 ],\n",
       "       [3.4034004],\n",
       "       [4.54526  ],\n",
       "       [4.5444226],\n",
       "       [3.4032607],\n",
       "       [4.5414686],\n",
       "       [4.5447006],\n",
       "       [4.5413322],\n",
       "       [3.4033816],\n",
       "       [4.543948 ],\n",
       "       [3.4034443],\n",
       "       [3.4031596],\n",
       "       [4.495387 ],\n",
       "       [4.5237627],\n",
       "       [3.5864112],\n",
       "       [3.4033704],\n",
       "       [4.545221 ],\n",
       "       [3.4034019],\n",
       "       [3.4034095],\n",
       "       [3.4033992],\n",
       "       [3.525546 ],\n",
       "       [3.4035058],\n",
       "       [3.403352 ],\n",
       "       [4.5430255],\n",
       "       [3.5684876],\n",
       "       [4.5406084],\n",
       "       [3.4034407],\n",
       "       [4.499638 ],\n",
       "       [3.4034333],\n",
       "       [3.4035158],\n",
       "       [4.5391555],\n",
       "       [4.545115 ],\n",
       "       [3.4034243],\n",
       "       [3.4033525],\n",
       "       [4.5448885],\n",
       "       [4.5449634],\n",
       "       [3.463106 ],\n",
       "       [4.5008893],\n",
       "       [3.4032564],\n",
       "       [3.4033918],\n",
       "       [4.544686 ],\n",
       "       [3.4033196],\n",
       "       [3.4034576],\n",
       "       [3.4034727],\n",
       "       [3.4293978],\n",
       "       [3.4034278],\n",
       "       [3.40342  ],\n",
       "       [4.5428386],\n",
       "       [4.4956183],\n",
       "       [3.403208 ],\n",
       "       [3.403467 ],\n",
       "       [3.4034479],\n",
       "       [3.4034047],\n",
       "       [3.403408 ],\n",
       "       [3.4038997],\n",
       "       [3.4377618],\n",
       "       [4.5418396],\n",
       "       [3.4036798],\n",
       "       [4.544813 ],\n",
       "       [3.403394 ],\n",
       "       [3.4032927],\n",
       "       [3.4032302],\n",
       "       [3.40341  ],\n",
       "       [3.4034448],\n",
       "       [3.4046676],\n",
       "       [3.4034016],\n",
       "       [4.5449896],\n",
       "       [4.536072 ],\n",
       "       [3.4047418],\n",
       "       [3.4035556],\n",
       "       [4.544949 ],\n",
       "       [4.5437956],\n",
       "       [4.5449758],\n",
       "       [3.4034173],\n",
       "       [3.4031293],\n",
       "       [3.4034576],\n",
       "       [3.4033418],\n",
       "       [3.4033787],\n",
       "       [4.544814 ],\n",
       "       [3.4034457],\n",
       "       [4.5448675],\n",
       "       [3.9542778],\n",
       "       [4.5405073],\n",
       "       [3.4033856],\n",
       "       [4.543114 ],\n",
       "       [3.4033344],\n",
       "       [3.4034557],\n",
       "       [3.4034739],\n",
       "       [4.5425453],\n",
       "       [3.928757 ],\n",
       "       [3.403408 ],\n",
       "       [4.534568 ],\n",
       "       [3.8343384],\n",
       "       [3.403334 ],\n",
       "       [4.5427847],\n",
       "       [3.4033813],\n",
       "       [3.403263 ],\n",
       "       [3.403291 ],\n",
       "       [4.541365 ],\n",
       "       [3.4034433],\n",
       "       [3.403381 ],\n",
       "       [3.403436 ],\n",
       "       [4.544595 ],\n",
       "       [3.4033787],\n",
       "       [3.4033966],\n",
       "       [3.4035184],\n",
       "       [4.5450416],\n",
       "       [4.5448794],\n",
       "       [3.4034712],\n",
       "       [3.4034073],\n",
       "       [3.5525873],\n",
       "       [3.4033735],\n",
       "       [3.4033859],\n",
       "       [4.541446 ],\n",
       "       [4.4447565],\n",
       "       [3.4033985],\n",
       "       [3.403395 ],\n",
       "       [3.403311 ],\n",
       "       [3.403363 ],\n",
       "       [3.403394 ],\n",
       "       [4.545282 ],\n",
       "       [4.5450306],\n",
       "       [3.4034464],\n",
       "       [4.541493 ],\n",
       "       [4.544931 ],\n",
       "       [3.4033513],\n",
       "       [3.4034398],\n",
       "       [3.4035003],\n",
       "       [3.617992 ],\n",
       "       [4.545294 ],\n",
       "       [4.545244 ],\n",
       "       [4.5078006],\n",
       "       [4.543099 ],\n",
       "       [3.4033592],\n",
       "       [4.5448937],\n",
       "       [3.4033828],\n",
       "       [3.403423 ],\n",
       "       [3.4034276],\n",
       "       [3.403449 ],\n",
       "       [4.544859 ],\n",
       "       [4.545112 ],\n",
       "       [3.4033382],\n",
       "       [4.545132 ],\n",
       "       [3.4034686],\n",
       "       [4.5427814],\n",
       "       [4.544459 ],\n",
       "       [3.4033873],\n",
       "       [4.5372615],\n",
       "       [3.4033904],\n",
       "       [3.4034154],\n",
       "       [4.5399837],\n",
       "       [4.5451922],\n",
       "       [4.5441003],\n",
       "       [3.4035087],\n",
       "       [4.5418415],\n",
       "       [3.4034178],\n",
       "       [4.5444574],\n",
       "       [3.4034302],\n",
       "       [3.4033663],\n",
       "       [3.4033957],\n",
       "       [3.403357 ],\n",
       "       [4.544657 ],\n",
       "       [3.4033842],\n",
       "       [3.403456 ],\n",
       "       [3.4033952],\n",
       "       [3.403409 ],\n",
       "       [4.5445976],\n",
       "       [3.4447565],\n",
       "       [3.403394 ],\n",
       "       [3.403384 ],\n",
       "       [3.4033775],\n",
       "       [4.5407157],\n",
       "       [4.5409765],\n",
       "       [4.545262 ],\n",
       "       [4.5427737],\n",
       "       [4.518506 ],\n",
       "       [3.403445 ],\n",
       "       [3.4034538],\n",
       "       [3.4033246],\n",
       "       [4.5396647],\n",
       "       [3.403479 ],\n",
       "       [3.403455 ],\n",
       "       [3.4050667],\n",
       "       [4.544068 ],\n",
       "       [4.5442343],\n",
       "       [3.4034026],\n",
       "       [3.4034426],\n",
       "       [3.4035003],\n",
       "       [4.5448785],\n",
       "       [3.4034095],\n",
       "       [3.4033983],\n",
       "       [3.4035695],\n",
       "       [4.545074 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/vocab.txt from cache at /home/sylva/.cache/huggingface/transformers/49c302ee103bf6737d0877cfbd658563cf4bbc4b7914363ca419ce8a3d8a4c51.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/special_tokens_map.json from cache at /home/sylva/.cache/huggingface/transformers/1e5909e4dfaa904617797ed35a6105a23daa56cbefca48fef329f772584699fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer_config.json from cache at /home/sylva/.cache/huggingface/transformers/29039dfe8c131360348e9f5ebecd464478cec7576c9af532b55ddcf9d4ec8d1e.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/pytorch_model.bin from cache at /home/sylva/.cache/huggingface/transformers/b774244369e464de2c660477b70bae7c3223fa7250aa1c8fc0b0f037ed58418a.087808d17814e241e9352c5ce0fea1a7d05e5b0f020d44b42b5f05922e96c923\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 3014\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='754' max='754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [754/754 01:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.912100</td>\n",
       "      <td>0.541974</td>\n",
       "      <td>0.548145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 754\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to out/output_group_01/checkpoint-500\n",
      "Configuration saved in out/output_group_01/checkpoint-500/config.json\n",
      "Model weights saved in out/output_group_01/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/output_group_01/checkpoint-500 (score: 0.5419739484786987).\n",
      "  4%|▍         | 1/25 [02:11<52:25, 131.08s/it]loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/vocab.txt from cache at /home/sylva/.cache/huggingface/transformers/49c302ee103bf6737d0877cfbd658563cf4bbc4b7914363ca419ce8a3d8a4c51.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/special_tokens_map.json from cache at /home/sylva/.cache/huggingface/transformers/1e5909e4dfaa904617797ed35a6105a23daa56cbefca48fef329f772584699fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer_config.json from cache at /home/sylva/.cache/huggingface/transformers/29039dfe8c131360348e9f5ebecd464478cec7576c9af532b55ddcf9d4ec8d1e.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/pytorch_model.bin from cache at /home/sylva/.cache/huggingface/transformers/b774244369e464de2c660477b70bae7c3223fa7250aa1c8fc0b0f037ed58418a.087808d17814e241e9352c5ce0fea1a7d05e5b0f020d44b42b5f05922e96c923\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 3013\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='754' max='754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [754/754 01:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.572160</td>\n",
       "      <td>0.573917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 754\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to out/output_group_02/checkpoint-500\n",
      "Configuration saved in out/output_group_02/checkpoint-500/config.json\n",
      "Model weights saved in out/output_group_02/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from out/output_group_02/checkpoint-500 (score: 0.5721600651741028).\n",
      "  8%|▊         | 2/25 [04:21<50:03, 130.60s/it]loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/vocab.txt from cache at /home/sylva/.cache/huggingface/transformers/49c302ee103bf6737d0877cfbd658563cf4bbc4b7914363ca419ce8a3d8a4c51.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/special_tokens_map.json from cache at /home/sylva/.cache/huggingface/transformers/1e5909e4dfaa904617797ed35a6105a23daa56cbefca48fef329f772584699fb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer_config.json from cache at /home/sylva/.cache/huggingface/transformers/29039dfe8c131360348e9f5ebecd464478cec7576c9af532b55ddcf9d4ec8d1e.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
      "loading file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/config.json from cache at /home/sylva/.cache/huggingface/transformers/ceb753d3f27a8c0d09184f35884666cda91b8ae610cd2a54d89793ac7663f1f9.13815020fd994b27db9974c0ce0ec4c47dfac6c8f11bf1a35a0a06d5b165665a\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/MiniLM-L12-H384-uncased/resolve/main/pytorch_model.bin from cache at /home/sylva/.cache/huggingface/transformers/b774244369e464de2c660477b70bae7c3223fa7250aa1c8fc0b0f037ed58418a.087808d17814e241e9352c5ce0fea1a7d05e5b0f020d44b42b5f05922e96c923\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 3012\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70/754 00:09 < 01:37, 7.04 it/s, Epoch 0.18/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [04:47<55:07, 143.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9032ec414d79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Train pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                             \u001b[0;31m# Revert to normal clipping otherwise, handling Apex or full precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m                             nn.utils.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                                 \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mclip_coef_clamped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for i in tqdm.tqdm(range(1, 26, 1)):\n",
    "    cur_group = \"group_\"+(\"%02d\" % i)\n",
    "    train_sub = train.loc[train[\"group\"] != cur_group]\n",
    "    \n",
    "    # train set\n",
    "    text_list = []\n",
    "    score_list = []\n",
    "    for index, row in data.iterrows():\n",
    "        s = 0\n",
    "        cnt = 0\n",
    "        for score in train_sub.loc[row[\"question_id\"] == train_sub[\"id\"]][\"rating\"]:\n",
    "            if not np.isnan(score):\n",
    "                s += score\n",
    "                cnt += 1\n",
    "        if cnt != 0:\n",
    "            text_list.append(row[\"question_text\"] + \" \" + row[\"reply_text\"])\n",
    "            score_list.append(s / cnt)\n",
    "    df = pd.DataFrame({\"text\": text_list, \"rating\": score_list}) \n",
    "\n",
    "    model_name = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "    X = list(df[\"text\"])\n",
    "    y = list(df[\"rating\"])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels:\n",
    "                item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "    val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        pred, labels = p\n",
    "        pred = pred.reshape((len(pred),))\n",
    "        return {\"r\": np.corrcoef(pred, labels)[0,1]}\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"out/output\"+\"_group_\"+(\"%02d\" % i),\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=1e-4,\n",
    "        num_train_epochs=2,\n",
    "        seed=0,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "\n",
    "    # Train pre-trained model\n",
    "    trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]loading configuration file out/output_group_01/checkpoint-500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/MiniLM-L12-H384-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file out/output_group_01/checkpoint-500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at out/output_group_01/checkpoint-500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 734\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='112' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92/92 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 77\n",
      "  Batch size = 8\n",
      "***** Running Prediction *****\n",
      "  Num examples = 75\n",
      "  Batch size = 8\n",
      "  4%|▍         | 1/25 [00:10<04:16, 10.67s/it]loading configuration file out/output_group_02/checkpoint-500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/MiniLM-L12-H384-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file out/output_group_02/checkpoint-500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at out/output_group_02/checkpoint-500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 747\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 64\n",
      "  Batch size = 8\n",
      "***** Running Prediction *****\n",
      "  Num examples = 59\n",
      "  Batch size = 8\n",
      "  8%|▊         | 2/25 [00:21<04:06, 10.72s/it]404 Client Error: Not Found for url: https://huggingface.co/out/output_group_03/checkpoint-500/resolve/main/config.json\n",
      "  8%|▊         | 2/25 [00:27<05:11, 13.56s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'out/output_group_03/checkpoint-500'. Make sure that:\n\n- 'out/output_group_03/checkpoint-500' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'out/output_group_03/checkpoint-500' is not a path to a local directory with something else, in that case)\n\n- or 'out/output_group_03/checkpoint-500' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             resolved_config_file = cached_path(\n\u001b[0m\u001b[1;32m    551\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1492\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1662\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1663\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1664\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/out/output_group_03/checkpoint-500/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-7eac4e33ecb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Load trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"out/output_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcur_group\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/checkpoint-500\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Define test trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   1249\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             logger.warn(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'out/output_group_03/checkpoint-500'. Make sure that:\n\n- 'out/output_group_03/checkpoint-500' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'out/output_group_03/checkpoint-500' is not a path to a local directory with something else, in that case)\n\n- or 'out/output_group_03/checkpoint-500' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "ret = {}\n",
    "for i in tqdm.tqdm(range(1, 26, 1)):\n",
    "    cur_group = \"group_\"+(\"%02d\" % i)\n",
    "    \n",
    "    # laod dev dataset\n",
    "    text_list1 = []\n",
    "    score_list1 = []\n",
    "    text_list2 = []\n",
    "    score_list2 = []\n",
    "    text_list3 = []\n",
    "    score_list3 = []\n",
    "    for index, row in data.iterrows():\n",
    "        df_subset = dev.loc[row[\"question_id\"] == dev[\"id\"]]\n",
    "        if cur_group in list(df_subset[\"group\"]):\n",
    "            s2 = 0\n",
    "            cnt2 = 0\n",
    "            s3 = 0\n",
    "            cnt3 = 0\n",
    "            for _, j in df_subset.iterrows():\n",
    "                if not np.isnan(score):\n",
    "                    if j[\"group\"] == cur_group:\n",
    "                        # set 2\n",
    "                        s2 += j[\"rating\"]\n",
    "                        cnt2 += 1\n",
    "                    else:\n",
    "                        # set 3\n",
    "                        s3 += score\n",
    "                        cnt3 += 1\n",
    "\n",
    "            if cnt2 != 0:\n",
    "                text_list2.append(row[\"question_text\"] + \" \" + row[\"reply_text\"])\n",
    "                score_list2.append(s2 / cnt2)\n",
    "            if cnt3 != 0:\n",
    "                text_list3.append(row[\"question_text\"] + \" \" + row[\"reply_text\"])\n",
    "                score_list3.append(s3 / cnt3)\n",
    "        else:\n",
    "            # set1\n",
    "            s = 0\n",
    "            cnt = 0\n",
    "            for score in df_subset[\"rating\"]:\n",
    "                if not np.isnan(score):\n",
    "                    s += score\n",
    "                    cnt += 1\n",
    "            if cnt != 0:\n",
    "                text_list1.append(row[\"question_text\"] + \" \" + row[\"reply_text\"])\n",
    "                score_list1.append(s / cnt)\n",
    "\n",
    "    X_test_tokenized1 = tokenizer(text_list1, padding=True, truncation=True, max_length=512)\n",
    "    X_test_tokenized2 = tokenizer(text_list2, padding=True, truncation=True, max_length=512)\n",
    "    X_test_tokenized3 = tokenizer(text_list3, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Create torch dataset\n",
    "    test_dataset1 = Dataset(X_test_tokenized1)\n",
    "    test_dataset2 = Dataset(X_test_tokenized2)\n",
    "    test_dataset3 = Dataset(X_test_tokenized3)\n",
    "\n",
    "    # Load trained model\n",
    "    model_path = \"out/output_\" + cur_group + \"/checkpoint-500\"\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
    "\n",
    "    # Define test trainer\n",
    "    test_trainer = Trainer(model)\n",
    "\n",
    "    # Make prediction\n",
    "    raw_pred1, _, _ = test_trainer.predict(test_dataset1)\n",
    "    raw_pred2, _, _ = test_trainer.predict(test_dataset2)\n",
    "    raw_pred3, _, _ = test_trainer.predict(test_dataset3)\n",
    "    \n",
    "    tmp = []\n",
    "    raw_pred1 = raw_pred1.reshape((len(raw_pred1),))\n",
    "    raw_pred2 = raw_pred2.reshape((len(raw_pred2),))\n",
    "    raw_pred3 = raw_pred3.reshape((len(raw_pred3),))\n",
    "\n",
    "    tmp.append(np.corrcoef(raw_pred1, np.array(score_list1))[0,1])\n",
    "    tmp.append(np.corrcoef(raw_pred2, np.array(score_list2))[0,1])\n",
    "    tmp.append(np.corrcoef(raw_pred3, np.array(score_list3))[0,1])\n",
    "    ret[cur_group] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group_01': [0.5793006007257147, 0.755119156853009, -0.04148565743116314],\n",
       " 'group_02': [0.5724079108338965, 0.7099946789968093, -0.24757961114789137]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e6c4b307fb65dceb1102a30a5dcde0f3e5b5942f46f4130f1acc6676b54ae88"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
