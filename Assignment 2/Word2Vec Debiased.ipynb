{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiased Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc262561df0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Handy command-line argument parsing\n",
    "import argparse\n",
    "\n",
    "# Sort of smart tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class to hold the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # These state variables become populated with function calls\n",
    "        #\n",
    "        # 1. load_data()\n",
    "        # 2. generate_negative_sampling_table()\n",
    "        #\n",
    "        # See those functions for how the various values get filled in\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "\n",
    "        # A utility data structure that lets us quickly sample \"negative\"\n",
    "        # instances in a context. This table contains unique-ids\n",
    "        self.negative_sampling_table = []\n",
    "        \n",
    "        # The dataset we'll use for training, as a sequence of unqiue word\n",
    "        # ids. This is the sequence across all documents after tokens have been\n",
    "        # randomly subsampled by the word2vec preprocessing step\n",
    "        self.full_token_sequence_as_ids = None\n",
    "\n",
    "        # Optional Task 1: Modeling Multi-word Expressions\n",
    "        self.mwes = {}\n",
    "\n",
    "        # Optional Task 4: Incorporating Synonyms\n",
    "        self.synonyms = {}\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        '''\n",
    "        Tokenize the document and returns a list of the tokens\n",
    "        '''\n",
    "        return self.tokenizer.tokenize(text)        \n",
    "\n",
    "    def load_data(self, file_name, min_token_freq, mwes_file, synonyms_file):\n",
    "        '''\n",
    "        Reads the data from the specified file as long long sequence of text\n",
    "        (ignoring line breaks) and populates the data structures of this\n",
    "        word2vec object.\n",
    "        '''\n",
    "\n",
    "        # Step 1: Read in the file and create a long sequence of tokens for\n",
    "        # all tokens in the file\n",
    "        all_tokens = []\n",
    "        print('Reading data and tokenizing')\n",
    "        with open(file_name, 'r', encoding='UTF-8') as file:\n",
    "            bios = file.read()\n",
    "        all_tokens = self.tokenize(bios)\n",
    "        all_tokens = [x.lower() for x in all_tokens]\n",
    "\n",
    "        # Step 1.1: Read the Multi-word Expressions file and group \n",
    "        # multi-world expressions into a single token\n",
    "        self.load_mwes(mwes_file)\n",
    "        for i, token in enumerate(all_tokens):\n",
    "            if token in self.mwes:\n",
    "                toGroup = True\n",
    "                for j in range(len(self.mwes[token])):\n",
    "                    if i + j + 1 >= len(all_tokens) or all_tokens[i + j + 1] != self.mwes[token][j]:\n",
    "                        toGroup = False\n",
    "                        break\n",
    "                if toGroup:\n",
    "                    for j in range(len(self.mwes[token])):\n",
    "                        all_tokens[i] += ' ' + self.mwes[token][j]\n",
    "                        all_tokens[i + j + 1] = '<REMOVE>'\n",
    "        all_tokens = [token for token in all_tokens if token != '<REMOVE>']    \n",
    "    \n",
    "        # Step 2: Count how many tokens we have of each type\n",
    "        print('Counting token frequencies')\n",
    "        self.word_counts = Counter(all_tokens)\n",
    "\n",
    "        # Step 3: Replace all tokens below the specified frequency with an <UNK>\n",
    "        # token. \n",
    "        #\n",
    "        # NOTE: You can do this step later if needed\n",
    "        print(\"Performing minimum thresholding\")\n",
    "        for i, token in enumerate(all_tokens):\n",
    "            if self.word_counts[token] < min_token_freq:\n",
    "                all_tokens[i] = '<UNK>'\n",
    "\n",
    "        # Step 4: update self.word_counts to be the number of times each word\n",
    "        # occurs (including <UNK>)\n",
    "        self.word_counts = Counter(all_tokens)\n",
    "        \n",
    "        \n",
    "        # Step 5: Create the mappings from word to unique integer ID and the\n",
    "        # reverse mapping.\n",
    "        for i, token in enumerate(list(set(all_tokens))):\n",
    "            self.index_to_word[i] = token\n",
    "            self.word_to_index[token] = i\n",
    "\n",
    "        # Step 5.1: Read the Synonyms file\n",
    "        self.load_synonyms(synonyms_file)\n",
    "        \n",
    "        # Step 6: Compute the probability of keeping any particular *token* of a\n",
    "        # word in the training sequence, which we'll use to subsample. This subsampling\n",
    "        # avoids having the training data be filled with many overly common words\n",
    "        # as positive examples in the context\n",
    "        total_count = sum(self.word_counts.values())\n",
    "        word_to_sample_prob = {word: (np.sqrt((count / total_count) / 0.001) + 1) * 0.001 / (count / total_count) for word, count in self.word_counts.items()}\n",
    "                        \n",
    "        # Step 7: process the list of tokens (after min-freq filtering) to fill\n",
    "        # a new list self.full_token_sequence_as_ids where \n",
    "        #\n",
    "        # (1) we probabilistically choose whether to keep each *token* based on the\n",
    "        # subsampling probabilities (note that this does not mean we drop\n",
    "        # an entire word!) and \n",
    "        #\n",
    "        # (2) all tokens are convered to their unique ids for faster training.\n",
    "        #\n",
    "        # NOTE: You can skip the subsampling part and just do step 2 to get\n",
    "        # your model up and running.\n",
    "            \n",
    "        # NOTE 2: You will perform token-based subsampling based on the probabilities in\n",
    "        # word_to_sample_prob. When subsampling, you are modifying the sequence itself \n",
    "        # (like deleting an item in a list). This action effectively makes the context\n",
    "        # window  larger for some target words by removing context words that are common\n",
    "        # from a particular context before the training occurs (which then would now include\n",
    "        # other words that were previously just outside the window).\n",
    "        token_ids = []\n",
    "        for token in all_tokens:\n",
    "            if word_to_sample_prob[token] < np.random.random():\n",
    "                continue\n",
    "            token_ids.append(self.word_to_index[token])\n",
    "        self.full_token_sequence_as_ids = token_ids\n",
    "\n",
    "        # Helpful print statement to verify what you've loaded\n",
    "        print('Loaded all data from %s; saw %d tokens (%d unique)' \\\n",
    "              % (file_name, len(self.full_token_sequence_as_ids),\n",
    "                 len(self.word_to_index)))\n",
    "\n",
    "\n",
    "    def load_mwes(self, mwes_file):\n",
    "        with open(mwes_file, 'r', encoding='UTF-8') as file:\n",
    "            mwes = file.read().splitlines()\n",
    "\n",
    "        for mwe in mwes:\n",
    "            word_list = self.tokenize(mwe)\n",
    "            word_list = [x.lower() for x in word_list]\n",
    "            self.mwes[word_list[0]] = word_list[1:]\n",
    "    \n",
    "\n",
    "    def load_synonyms(self, synonyms_file):\n",
    "        with open(synonyms_file, 'r', encoding='UTF-8') as file:\n",
    "            synonyms = file.read().splitlines()\n",
    "\n",
    "        for synonym in synonyms:\n",
    "            word_list = self.tokenize(synonym)\n",
    "            word_list = [x.lower() for x in word_list]\n",
    "            word_list_index = []\n",
    "            for word in word_list:\n",
    "                if word in self.word_to_index:\n",
    "                    word_list_index.append(self.word_to_index[word])\n",
    "            for index in word_list_index:\n",
    "                self.synonyms[index] = word_list_index\n",
    "        \n",
    "\n",
    "    def generate_negative_sampling_table(self, exp_power=0.75, table_size=1e6):\n",
    "        '''\n",
    "        Generates a big list data structure that we can quickly randomly index into\n",
    "        in order to select a negative training example (i.e., a word that was\n",
    "        *not* present in the context). \n",
    "        '''       \n",
    "        \n",
    "        # Step 1: Figure out how many instances of each word need to go into the\n",
    "        # negative sampling table. \n",
    "        #\n",
    "        # HINT: np.power and np.fill might be useful here        \n",
    "        print(\"Generating sampling table\")\n",
    "        powers = np.full(len(self.word_to_index), exp_power, dtype=float)\n",
    "        counts = [self.word_counts[word] for word in self.word_to_index]\n",
    "        probs = np.power(counts, powers)\n",
    "        probs /= sum(probs)\n",
    "\n",
    "        # Step 2: Create the table to the correct size. You'll want this to be a\n",
    "        # numpy array of type int\n",
    "        self.negative_sampling_table = [0] * len(self.word_to_index)\n",
    "\n",
    "        # Step 3: Fill the table so that each word has a number of IDs\n",
    "        # proportionate to its probability of being sampled.\n",
    "        #\n",
    "        # Example: if we have 3 words \"a\" \"b\" and \"c\" with probabilites 0.5,\n",
    "        # 0.33, 0.16 and a table size of 6 then our table would look like this\n",
    "        # (before converting the words to IDs):\n",
    "        #\n",
    "        # [ \"a\", \"a\", \"a\", \"b\", \"b\", \"c\" ]\n",
    "        #\n",
    "        self.negative_sampling_table = np.random.choice(len(self.word_to_index), int(table_size), p=probs)\n",
    "\n",
    "\n",
    "    def generate_negative_samples(self, cur_context_word_id, num_samples):\n",
    "        '''\n",
    "        Randomly samples the specified number of negative samples from the lookup\n",
    "        table and returns this list of IDs as a numpy array. As a performance\n",
    "        improvement, avoid sampling a negative example that has the same ID as\n",
    "        the current positive context word.\n",
    "        '''\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Create a list and sample from the negative_sampling_table to\n",
    "        # grow the list to num_samples, avoiding adding a negative example that\n",
    "        # has the same ID as the current context_word\n",
    "        for _ in range(num_samples):\n",
    "            sample = np.random.choice(self.negative_sampling_table)\n",
    "            while sample == cur_context_word_id:\n",
    "                sample = np.random.choice(self.negative_sampling_table)\n",
    "            results.append(sample)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n",
      "Loaded all data from wiki-bios.med.txt; saw 17453348 tokens (100640 unique)\n",
      "Generating sampling table\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "corpus.load_data('wiki-bios.med.txt', 5, 'bio-mwes.txt', 'synonyms.txt')\n",
    "corpus.generate_negative_sampling_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "num_negative_samples_per_target = 2\n",
    "\n",
    "training_data = []\n",
    "    \n",
    "# Loop through each token in the corpus and generate an instance for each, \n",
    "# adding it to training_data\n",
    "for target_word_index, target_word_id in enumerate(corpus.full_token_sequence_as_ids):\n",
    "\n",
    "    if corpus.index_to_word[target_word_id] == '<UNK>':\n",
    "        continue\n",
    "\n",
    "    if target_word_id in corpus.synonyms:\n",
    "        target_word_id = np.random.choice(corpus.synonyms[target_word_id])\n",
    "\n",
    "    # For exach target word in our dataset, select context words \n",
    "    # within +/- the window size in the token sequence\n",
    "    if target_word_index < window_size:\n",
    "        window_indice = list(range(-target_word_index, 0)) + list(range(1, window_size + 1))\n",
    "    elif target_word_index > len(corpus.full_token_sequence_as_ids) - window_size - 1:\n",
    "        window_indice = list(range(-window_size, 0)) + list(range(1, len(corpus.full_token_sequence_as_ids) - target_word_index))\n",
    "    else:\n",
    "        window_indice = list(range(-window_size, 0)) + list(range(1, window_size + 1))\n",
    "\n",
    "    context_ids = [corpus.full_token_sequence_as_ids[target_word_index + i] for i in window_indice]\n",
    "    predicted_labels = [1 for _ in window_indice]\n",
    "    \n",
    "    # For each positive target, we need to select negative examples of\n",
    "    # words that were not in the context. Use the num_negative_samples_per_target\n",
    "    # hyperparameter to generate these, using the generate_negative_samples()\n",
    "    # method from the Corpus class\n",
    "    negative_ids = []\n",
    "    for context_id in context_ids:\n",
    "        negative_ids += corpus.generate_negative_samples(context_id, num_negative_samples_per_target)\n",
    "        predicted_labels += [0] * num_negative_samples_per_target\n",
    "\n",
    "    # NOTE: this part might not make sense until later when you do the training \n",
    "    # so feel free to revisit it to see why it happens.\n",
    "    #\n",
    "    # Our training will use batches of instances together (compare that \n",
    "    # with HW1's SGD that used one item at a time). PyTorch will require\n",
    "    # that all instances in a batches have the same size, which creates an issue\n",
    "    # for us here since the target wordss at the very beginning or end of the corpus\n",
    "    # have shorter contexts. \n",
    "    # \n",
    "    # To work around these edge-cases, we need to ensure that each instance has\n",
    "    # the same size, which means it needs to have the same number of positive\n",
    "    # and negative examples. Since we are short on positive examples here (due\n",
    "    # to the edge of the corpus), we can just add more negative samples.\n",
    "    #\n",
    "    # YOUR TASK: determine what is the maximum number of context words (positive\n",
    "    # and negative) for any instance and then, for instances that have fewer than\n",
    "    # this number of context words, add in negative examples.\n",
    "    #\n",
    "    # NOTE: The maximum is fixed, so you can precompute this outside the loop\n",
    "    # ahead of time.\n",
    "    while len(predicted_labels) < window_size * 6:\n",
    "        context_id = np.random.choice(context_ids)\n",
    "        negative_ids += [np.random.choice(corpus.generate_negative_samples(context_id, num_negative_samples_per_target))]\n",
    "        predicted_labels += [0]\n",
    "    \n",
    "    target_word_id = np.array([target_word_id])\n",
    "    word_ids = np.array(context_ids + negative_ids)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "    training_data += [(target_word_id, word_ids, predicted_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Save what state you want and create the embeddings for your\n",
    "        # target and context words\n",
    "        self.target_embeddings = None\n",
    "        self.context_embeddings = None\n",
    "        \n",
    "        # Once created, let's fill the embeddings with non-zero random\n",
    "        # numbers. We need to do this to get the training started. \n",
    "        #\n",
    "        # NOTE: Why do this? Think about what happens if all the embeddings\n",
    "        # are all zeros initially. What would the predictions look like for\n",
    "        # word2vec with these embeddings and how would the updated work?\n",
    "        \n",
    "        self.init_emb(init_range=0.5/self.vocab_size)\n",
    "        \n",
    "    def init_emb(self, init_range):\n",
    "        \n",
    "        # Fill your two embeddings with random numbers uniformly sampled\n",
    "        # between +/- init_range\n",
    "        W = torch.tensor(np.random.uniform(-init_range, init_range, [self.vocab_size, self.embedding_size]), dtype = torch.float32)\n",
    "        self.target_embeddings = nn.Embedding.from_pretrained(W).requires_grad_(True)\n",
    "        C = torch.tensor(np.random.uniform(-init_range, init_range, [self.vocab_size, self.embedding_size]), dtype = torch.float32)\n",
    "        self.context_embeddings = nn.Embedding.from_pretrained(C).requires_grad_(True)\n",
    "        \n",
    "    def forward(self, target_word_id, context_word_ids):\n",
    "        ''' \n",
    "        Predicts whether each context word was actually in the context of the target word.\n",
    "        The input is a tensor with a single target word's id and a tensor containing each\n",
    "        of the context words' ids (this includes both positive and negative examples).\n",
    "        '''\n",
    "        \n",
    "        # NOTE 1: This is probably the hardest part of the homework, so you'll\n",
    "        # need to figure out how to do the dot-product between embeddings and return\n",
    "        # the sigmoid. Be prepared for lots of debugging. For some reference,\n",
    "        # our implementation is three lines and really the hard part is just\n",
    "        # the last line. However, it's usually a matter of figuring out what\n",
    "        # that one line looks like that ends up being the hard part.\n",
    "        \n",
    "        # NOTE 2: In this homework you'll be dealing with *batches* of instances\n",
    "        # rather than a single instance at once. PyTorch mostly handles this\n",
    "        # seamlessly under the hood for you (which is very nice) but batching\n",
    "        # can show in weird ways and create challenges in debugging initially.\n",
    "        # For one, your inputs will get an extra dimension. So, for example,\n",
    "        # if you have a batch size of 4, your input for target_word_id will\n",
    "        # really be 4 x 1. If you get the embeddings of those targets,\n",
    "        # it then becomes 4x50! The same applies to the context_word_ids, except\n",
    "        # that was alreayd a list so now you have things with shape \n",
    "        #\n",
    "        #    (batch x context_words x embedding_size)\n",
    "        #\n",
    "        # One of your tasks will be to figure out how to get things lined up\n",
    "        # so everything \"just works\". When it does, the code looks surprisingly\n",
    "        # simple, but it might take a lot of debugging (or not!) to get there.\n",
    "        \n",
    "        # NOTE 3: We *strongly* discourage you from looking for existing \n",
    "        # implementations of word2vec online. Sadly, having reviewed most of the\n",
    "        # highly-visible ones, they are actually wrong (wow!) or are doing\n",
    "        # inefficient things like computing the full softmax instead of doing\n",
    "        # the negative sampling. Looking at these will likely leave you more\n",
    "        # confused than if you just tried to figure it out yourself.\n",
    "        \n",
    "        # NOTE 4: There many ways to implement this, some more efficient\n",
    "        # than others. You will want to get it working first and then\n",
    "        # test the timing to see how long it takes. As long as the\n",
    "        # code works (vector comparisons look good) you'll receive full\n",
    "        # credit. However, very slow implementations may take hours(!)\n",
    "        # to converge so plan ahead.\n",
    "        \n",
    "        \n",
    "        # Hint 1: You may want to review the mathematical operations on how\n",
    "        # to compute the dot product to see how to do these\n",
    "        \n",
    "        # Hint 2: the \"dim\" argument for some operations may come in handy,\n",
    "        # depending on your implementation\n",
    "        \n",
    "        # Hint 3: printing the shape of the tensors can come in very handy when\n",
    "        # debugging to see where things aren't lining up\n",
    "           \n",
    "        # TODO: Implement the forward pass of word2vec\n",
    "        v_t = self.target_embeddings(target_word_id)\n",
    "        v_c = self.target_embeddings(context_word_ids)\n",
    "        x = torch.matmul(v_t, torch.transpose(v_c, 1, 2))\n",
    "        outputs = torch.sigmoid(x)\n",
    "        return torch.squeeze(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    '''\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]))\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]))\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().numpy(),\n",
    "                                      embedding_two.detach().numpy())))\n",
    "    return similarity\n",
    "\n",
    "def some_bias_measuring_function(model):\n",
    "    women_men = torch.tensor([compute_cosine_similarity(model, corpus.word_to_index, \"men\", \"women\")], dtype = torch.float32, requires_grad=True)\n",
    "    woman_man = torch.tensor([compute_cosine_similarity(model, corpus.word_to_index, \"man\", \"woman\")], dtype = torch.float32, requires_grad=True)\n",
    "\n",
    "    bias1 = torch.tensor([1]) - torch.max(torch.abs(women_men), torch.tensor([0]))\n",
    "    bias2 = torch.tensor([1]) - torch.max(torch.abs(woman_man), torch.tensor([0]))\n",
    "\n",
    "    return torch.max(bias1, bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67704it [1:40:50, 11.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (target_embeddings): Embedding(100640, 50)\n",
       "  (context_embeddings): Embedding(100640, 50)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Set your training stuff, hyperparameters, models, tensorboard writer etc. here\n",
    "model = Word2Vec(len(corpus.word_to_index), 50)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_function = nn.BCELoss()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "min_loss = 100\n",
    "min_bias = 100\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    loss_sum = 0\n",
    "    bias_sum = 0\n",
    "\n",
    "    # TODO: use your DataLoader to iterate over the data\n",
    "    for step, data in tqdm(enumerate(DataLoader(training_data, batch_size=256, shuffle=True))):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # NOTE: since you created the data as a tuple of three np.array instances,\n",
    "        # these have now been converted to Tensor objects for us\n",
    "        target_ids, context_ids, labels = data\n",
    "        labels = labels.float()\n",
    "        \n",
    "        # TODO: Fill in all the training details here\n",
    "        outputs = model(target_ids, context_ids)\n",
    "        loss1 = loss_function(outputs, labels)\n",
    "        loss2 = some_bias_measuring_function(model)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to tensorboard. Be sure\n",
    "        # to reset the running sum after reporting it.\n",
    "        \n",
    "        loss_sum += loss1.item()\n",
    "        bias_sum += loss2.item()\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            if bias_sum < min_bias and loss_sum < min_loss:\n",
    "                min_loss = loss_sum\n",
    "                min_bias = bias_sum\n",
    "                torch.save(model, 'bios_med_batch_256_debiased.pth')\n",
    "            writer.add_scalar('Debiased/Loss', loss_sum, step + 1)\n",
    "            writer.add_scalar('Debiased/Bias', bias_sum, step + 1)\n",
    "            loss_sum = 0\n",
    "            bias_sum = 0\n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "writer.close()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/debiased-bias.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "img1 = 'img/debiased-bias.png'\n",
    "Image(url=img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/debiased-loss.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2 = 'img/debiased-loss.png'\n",
    "Image(url=img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "    kv = KeyedVectors(vector_size=model.embedding_size)        \n",
    "    vectors = []\n",
    "    words = []\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        vectors.append(model.target_embeddings(torch.LongTensor([index])).detach().numpy()[0])\n",
    "        words.append(word.replace(' ', '_'))\n",
    "\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100640/100640 [00:37<00:00, 2664.96it/s] \n"
     ]
    }
   ],
   "source": [
    "save(model, corpus, 'bios_med_batch_256_debiased.kv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
